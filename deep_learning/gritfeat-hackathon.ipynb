{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:21:40.164773Z","iopub.status.busy":"2023-01-28T06:21:40.164362Z","iopub.status.idle":"2023-01-28T06:21:41.296666Z","shell.execute_reply":"2023-01-28T06:21:41.295629Z","shell.execute_reply.started":"2023-01-28T06:21:40.164738Z"},"trusted":true},"outputs":[],"source":["# import required libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import glob\n","import plotly.graph_objects as go\n","import cv2\n","from PIL import Image\n","from PIL import ImageFile\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.np_utils import to_categorical\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten , Dropout\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"markdown","metadata":{},"source":["### Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:35.236054Z","iopub.status.busy":"2023-01-28T06:25:35.235043Z","iopub.status.idle":"2023-01-28T06:25:35.275311Z","shell.execute_reply":"2023-01-28T06:25:35.274196Z","shell.execute_reply.started":"2023-01-28T06:25:35.236015Z"},"trusted":true},"outputs":[],"source":["root_dir = '../input/intel-mobileodt-cervical-cancer-screening'\n","train_dir = os.path.join(root_dir,'train', 'train')\n","\n","type1_dir = os.path.join(train_dir, 'Type_1')\n","type2_dir = os.path.join(train_dir, 'Type_2')\n","type3_dir = os.path.join(train_dir, 'Type_3')\n","\n","train_type1_files = glob.glob(type1_dir+'/*.jpg')\n","train_type2_files = glob.glob(type2_dir+'/*.jpg')\n","train_type3_files = glob.glob(type3_dir+'/*.jpg')\n","\n","added_type1_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_1_v2\", \"Type_1\")+'/*.jpg')\n","added_type2_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_2_v2\", \"Type_2\")+'/*.jpg')\n","added_type3_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_3_v2\", \"Type_3\")+'/*.jpg')\n","\n","\n","type1_files = train_type1_files + added_type1_files\n","type2_files = train_type2_files + added_type2_files\n","type3_files = train_type3_files + added_type3_files\n","\n","print(f'''Type 1 files for training: {len(train_type1_files)} \n","Type 2 files for training: {len(train_type2_files)}\n","Type 3 files for training: {len(train_type3_files)}''' )\n","\n","print(f'''Added Type 1 files for training: {len(added_type1_files)} \n","Added Type 2 files for training: {len(added_type2_files)}\n","Added Type 3 files for training: {len(added_type3_files)}''' )\n","\n","print(f'''Type 1 files for training: {len(type1_files)} \n","Type 2 files for training: {len(type2_files)}\n","Type 3 files for training: {len(type3_files)}''' )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:36.843762Z","iopub.status.busy":"2023-01-28T06:25:36.842955Z","iopub.status.idle":"2023-01-28T06:25:36.864581Z","shell.execute_reply":"2023-01-28T06:25:36.862907Z","shell.execute_reply.started":"2023-01-28T06:25:36.843723Z"},"trusted":true},"outputs":[],"source":["# # create dataframe of file and labels\n","files = {'filepath': type1_files + type2_files + type3_files,\n","          'label': ['Type 1']* len(type1_files) + ['Type 2']* len(type2_files) + ['Type 3']* len(type3_files)}\n","\n","files_df = pd.DataFrame(files).sample(frac=1, random_state= 1).reset_index(drop=True)\n","files_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:37.997185Z","iopub.status.busy":"2023-01-28T06:25:37.996033Z","iopub.status.idle":"2023-01-28T06:25:42.035842Z","shell.execute_reply":"2023-01-28T06:25:42.034866Z","shell.execute_reply.started":"2023-01-28T06:25:37.997136Z"},"trusted":true},"outputs":[],"source":["# display sample images of types\n","for label in ('Type 1', 'Type 2', 'Type 3'):\n","    filepaths = files_df[files_df['label']==label]['filepath'].values[:5]\n","    fig = plt.figure(figsize= (15, 6))\n","    for i, path in enumerate(filepaths):\n","        img = cv2.imread(path)\n","        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","        img = cv2.resize(img, (224, 224))\n","        fig.add_subplot(1, 5, i+1)\n","        plt.imshow(img)\n","        plt.subplots_adjust(hspace=0.5)\n","        plt.axis(False)\n","        plt.title(label)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Processing"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:42.038736Z","iopub.status.busy":"2023-01-28T06:25:42.037778Z","iopub.status.idle":"2023-01-28T06:25:42.059820Z","shell.execute_reply":"2023-01-28T06:25:42.058713Z","shell.execute_reply.started":"2023-01-28T06:25:42.038694Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6572 821 822\n"]}],"source":["#  split the data into train  and validation set\n","train_df, eval_df = train_test_split(files_df, test_size= 0.2, stratify= files_df['label'], random_state= 1)\n","val_df, test_df = train_test_split(eval_df, test_size= 0.5, stratify= eval_df['label'], random_state= 1)\n","print(len(train_df), len(val_df), len(test_df))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:42.061988Z","iopub.status.busy":"2023-01-28T06:25:42.061596Z","iopub.status.idle":"2023-01-28T06:25:42.069251Z","shell.execute_reply":"2023-01-28T06:25:42.067994Z","shell.execute_reply.started":"2023-01-28T06:25:42.061937Z"},"trusted":true},"outputs":[],"source":["# loads images from dataframe\n","def load_images(dataframe):\n","    features = []\n","    filepaths = dataframe['filepath'].values\n","    labels = dataframe['label'].values\n","    \n","    for path in filepaths:\n","        img = cv2.imread(path)\n","        if img is None:\n","            print(dataframe[dataframe['filepath'] == path])\n","            dataframe.drop(dataframe[dataframe['filepath'] == path].index, inplace = True)\n","            continue\n","        resized_img = cv2.resize(img, (180, 180))\n","        features.append(np.array(resized_img))\n","    return np.array(features), np.array( dataframe['label'].values)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:42.072940Z","iopub.status.busy":"2023-01-28T06:25:42.072277Z","iopub.status.idle":"2023-01-28T06:25:42.079515Z","shell.execute_reply":"2023-01-28T06:25:42.078366Z","shell.execute_reply.started":"2023-01-28T06:25:42.072894Z"},"trusted":true},"outputs":[],"source":["# initially we loaded from dataset but later we saved it into pickle and loaded from it \n","# pickle contained all the transformed low res images, while the files contained all the high res images \n","# train_features, train_labels = load_images(train_df)\n","# val_features, val_labels = load_images(val_df)\n","# test_features, test_labels = load_images(test_df)"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-01-28T06:25:42.081707Z","iopub.status.busy":"2023-01-28T06:25:42.081319Z","iopub.status.idle":"2023-01-28T06:25:49.987020Z","shell.execute_reply":"2023-01-28T06:25:49.985937Z","shell.execute_reply.started":"2023-01-28T06:25:42.081672Z"},"trusted":true},"outputs":[],"source":["import pickle\n","train_features, train_labels = None, None\n","with open('/kaggle/input/dataset-project/dataset/train.pickle', 'rb') as handle:\n","    train_features, train_labels = pickle.load(handle)\n","\n","val_features, val_labels = None, None\n","with open('/kaggle/input/dataset-project/dataset/val.pickle', 'rb') as handle:\n","    val_features, val_labels = pickle.load(handle)\n","\n","test_features, test_labels = None, None\n","with open('/kaggle/input/dataset-project/dataset/test.pickle', 'rb') as handle:\n","    test_features, test_labels = pickle.load(handle)    "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:49.989675Z","iopub.status.busy":"2023-01-28T06:25:49.989264Z","iopub.status.idle":"2023-01-28T06:25:49.996072Z","shell.execute_reply":"2023-01-28T06:25:49.994807Z","shell.execute_reply.started":"2023-01-28T06:25:49.989638Z"},"trusted":true},"outputs":[],"source":["# import pickle \n","# with open('test.pickle', 'wb') as handle:\n","#     pickle.dump((test_features, test_labels), handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:49.998551Z","iopub.status.busy":"2023-01-28T06:25:49.997612Z","iopub.status.idle":"2023-01-28T06:25:50.008812Z","shell.execute_reply":"2023-01-28T06:25:50.007747Z","shell.execute_reply.started":"2023-01-28T06:25:49.998524Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(6569, 6569, 822, 822, 822, 822)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# check lengths of training and evaluation  sets\n","len(train_features), len(train_labels), len(test_features), len(test_labels), len(test_features), len(test_labels) "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:50.010936Z","iopub.status.busy":"2023-01-28T06:25:50.010473Z","iopub.status.idle":"2023-01-28T06:25:50.019088Z","shell.execute_reply":"2023-01-28T06:25:50.018020Z","shell.execute_reply.started":"2023-01-28T06:25:50.010886Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(180, 180, 3)\n"]}],"source":["# get image shape\n","InputShape = train_features[0].shape\n","print(InputShape)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:50.021022Z","iopub.status.busy":"2023-01-28T06:25:50.020646Z","iopub.status.idle":"2023-01-28T06:25:52.012207Z","shell.execute_reply":"2023-01-28T06:25:52.011081Z","shell.execute_reply.started":"2023-01-28T06:25:50.020988Z"},"trusted":true},"outputs":[],"source":["# normalize the features\n","X_train = train_features/255\n","X_val  = val_features/255\n","X_test  = test_features/255"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:52.015795Z","iopub.status.busy":"2023-01-28T06:25:52.015493Z","iopub.status.idle":"2023-01-28T06:25:52.026348Z","shell.execute_reply":"2023-01-28T06:25:52.025092Z","shell.execute_reply.started":"2023-01-28T06:25:52.015768Z"},"trusted":true},"outputs":[],"source":["# encode the labels\n","le = LabelEncoder().fit(['Type 1', 'Type 2', 'Type 3'])\n","y_train = le.transform(train_labels)\n","y_val = le.transform(val_labels)\n","y_test = le.transform(test_labels)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:52.028933Z","iopub.status.busy":"2023-01-28T06:25:52.028342Z","iopub.status.idle":"2023-01-28T06:25:52.037655Z","shell.execute_reply":"2023-01-28T06:25:52.036511Z","shell.execute_reply.started":"2023-01-28T06:25:52.028893Z"},"trusted":true},"outputs":[],"source":["# initialize image data generator for training and evaluation sets\n","import numpy as np \n","import random\n","def add_noise(img):\n","    '''Add random noise to an image'''\n","    VARIABILITY = 50\n","    deviation = VARIABILITY*random.random()\n","    noise = np.random.normal(0, deviation, img.shape)\n","    img += noise\n","    np.clip(img, 0., 255.)\n","    return img\n","\n","train_datagen = ImageDataGenerator(\n","                                rotation_range = 40,\n","                                zoom_range = 0.2,\n","                                width_shift_range=0.2,\n","                                height_shift_range=0.2,\n","                                shear_range=0.2,\n","                                horizontal_flip=True,\n","                                vertical_flip = True)\n","\n","eval_datagen = ImageDataGenerator()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:52.041225Z","iopub.status.busy":"2023-01-28T06:25:52.040675Z","iopub.status.idle":"2023-01-28T06:25:53.290496Z","shell.execute_reply":"2023-01-28T06:25:53.289431Z","shell.execute_reply.started":"2023-01-28T06:25:52.041196Z"},"trusted":true},"outputs":[],"source":["# apply data augmentation to features\n","BATCH_SIZE= 12\n","train_gen = train_datagen.flow(X_train, y_train, batch_size= BATCH_SIZE)\n","val_gen = eval_datagen.flow(X_val, y_val, batch_size= BATCH_SIZE)\n","test_gen = eval_datagen.flow(X_test, y_test, batch_size= BATCH_SIZE)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:25:53.294717Z","iopub.status.busy":"2023-01-28T06:25:53.294394Z","iopub.status.idle":"2023-01-28T06:25:53.386164Z","shell.execute_reply":"2023-01-28T06:25:53.385111Z","shell.execute_reply.started":"2023-01-28T06:25:53.294689Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["data batch shape: (12, 180, 180, 3) \n"," labels batch shape: (12,)\n"]}],"source":["# show shape of each  batch\n","for data_batch, labels_batch in train_gen:\n","    print('data batch shape: {} \\n labels batch shape: {}'.format(data_batch.shape, labels_batch.shape))\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:15.222493Z","iopub.status.busy":"2023-01-28T06:26:15.222102Z","iopub.status.idle":"2023-01-28T06:26:15.402004Z","shell.execute_reply":"2023-01-28T06:26:15.400915Z","shell.execute_reply.started":"2023-01-28T06:26:15.222463Z"},"trusted":true},"outputs":[],"source":["## Train set distribution \n","labels = list(map(lambda x: int(x[-1]), train_labels))\n","plt.figure(figsize = (10, 10))\n","plt.bar([\"Type 1\", \"Type 2\", \"Type  3\"], [labels.count(1), labels.count(2), labels.count(3)])\n","# plt.title(\"\")\n","# plt.xlabel()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Model Training "]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:18.579625Z","iopub.status.busy":"2023-01-28T06:26:18.578693Z","iopub.status.idle":"2023-01-28T06:26:21.054197Z","shell.execute_reply":"2023-01-28T06:26:21.053110Z","shell.execute_reply.started":"2023-01-28T06:26:18.579578Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 2s 0us/step\n","58900480/58889256 [==============================] - 2s 0us/step\n"]}],"source":["# initialize pretrained vgg model base\n","conv_base = VGG16(weights= 'imagenet', include_top= False, input_shape= (180, 180, 3))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:21.057372Z","iopub.status.busy":"2023-01-28T06:26:21.056614Z","iopub.status.idle":"2023-01-28T06:26:21.064726Z","shell.execute_reply":"2023-01-28T06:26:21.063626Z","shell.execute_reply.started":"2023-01-28T06:26:21.057330Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["This is the number of trainable weights before freezing layers in the conv base: 26\n"]}],"source":["# show trainable layers before freezing\n","print('This is the number of trainable weights '\n","'before freezing layers in the conv base:', len(conv_base.trainable_weights))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:21.066781Z","iopub.status.busy":"2023-01-28T06:26:21.066409Z","iopub.status.idle":"2023-01-28T06:26:21.086734Z","shell.execute_reply":"2023-01-28T06:26:21.085521Z","shell.execute_reply.started":"2023-01-28T06:26:21.066746Z"},"trusted":true},"outputs":[],"source":["# freeze few layers of pretrained model\n","for layer in conv_base.layers[:-5]:\n","    layer.trainable= False"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:22.151440Z","iopub.status.busy":"2023-01-28T06:26:22.151057Z","iopub.status.idle":"2023-01-28T06:26:22.157601Z","shell.execute_reply":"2023-01-28T06:26:22.156421Z","shell.execute_reply.started":"2023-01-28T06:26:22.151409Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["This is the number of trainable weights after freezing layers in the conv base: 6\n"]}],"source":["# show trainable layers after freezing\n","print('This is the number of trainable weights '\n","'after freezing layers in the conv base:', len(conv_base.trainable_weights))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:22.930307Z","iopub.status.busy":"2023-01-28T06:26:22.927973Z","iopub.status.idle":"2023-01-28T06:26:23.002292Z","shell.execute_reply":"2023-01-28T06:26:23.001286Z","shell.execute_reply.started":"2023-01-28T06:26:22.930257Z"},"trusted":true},"outputs":[],"source":["# build model \n","import keras\n","METRICS = [\n","      keras.metrics.BinaryAccuracy(name='accuracy'),\n","#       keras.metrics.Precision(name='precision'),\n","#       keras.metrics.Recall(name='recall'),\n"," \n","      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n","]\n","\n","model = Sequential([conv_base, \n","                    Flatten(),\n","                    Dropout(0.5),\n","                    Dense(3, activation='softmax')])"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:23.814235Z","iopub.status.busy":"2023-01-28T06:26:23.813454Z","iopub.status.idle":"2023-01-28T06:26:23.822704Z","shell.execute_reply":"2023-01-28T06:26:23.821507Z","shell.execute_reply.started":"2023-01-28T06:26:23.814197Z"},"trusted":true},"outputs":[],"source":["from keras import backend as K\n","def precision(y_true, y_pred):\n","    \"\"\"Precision metric.\n","    Only computes a batch-wise average of precision.\n","    Computes the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    \"\"\"\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def recall(y_true, y_pred):\n","    \"\"\"Recall metric.\n","    Only computes a batch-wise average of recall.\n","    Computes the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    \"\"\"\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:25.928648Z","iopub.status.busy":"2023-01-28T06:26:25.927995Z","iopub.status.idle":"2023-01-28T06:26:25.944266Z","shell.execute_reply":"2023-01-28T06:26:25.942695Z","shell.execute_reply.started":"2023-01-28T06:26:25.928608Z"},"trusted":true},"outputs":[],"source":["# compile model\n","model.compile(optimizer= Adam(0.0001), loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:27.086718Z","iopub.status.busy":"2023-01-28T06:26:27.086341Z","iopub.status.idle":"2023-01-28T06:26:27.093747Z","shell.execute_reply":"2023-01-28T06:26:27.092623Z","shell.execute_reply.started":"2023-01-28T06:26:27.086688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 5, 5, 512)         14714688  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 12800)             0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 12800)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 3)                 38403     \n","=================================================================\n","Total params: 14,753,091\n","Trainable params: 7,117,827\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n"]}],"source":["# show model summary\n","model.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:27.936877Z","iopub.status.busy":"2023-01-28T06:26:27.936074Z","iopub.status.idle":"2023-01-28T06:26:27.942095Z","shell.execute_reply":"2023-01-28T06:26:27.940729Z","shell.execute_reply.started":"2023-01-28T06:26:27.936837Z"},"trusted":true},"outputs":[],"source":["# define training steps\n","TRAIN_STEPS = len(train_labels)//BATCH_SIZE\n","VAL_STEPS = len(val_labels)//BATCH_SIZE"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:26:29.236950Z","iopub.status.busy":"2023-01-28T06:26:29.236074Z","iopub.status.idle":"2023-01-28T06:26:29.722745Z","shell.execute_reply":"2023-01-28T06:26:29.721725Z","shell.execute_reply.started":"2023-01-28T06:26:29.236895Z"},"trusted":true},"outputs":[],"source":["model.load_weights(\"/kaggle/input/cervical-weights/cervicalModel.weights.hdf5\")"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:29:41.127444Z","iopub.status.busy":"2023-01-28T06:29:41.127022Z","iopub.status.idle":"2023-01-28T06:29:41.134780Z","shell.execute_reply":"2023-01-28T06:29:41.133530Z","shell.execute_reply.started":"2023-01-28T06:29:41.127413Z"},"trusted":true},"outputs":[],"source":["# initialize callbacks\n","reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=10, verbose= 1, mode='min', factor=  0.2, min_lr = 1e-5)\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience = 10, verbose=1, mode='max', restore_best_weights= True)\n","\n","checkpoint = ModelCheckpoint('cervicalModel_weights.hdf5', monitor='val_accuracy', verbose=1,save_best_only=True, mode= 'max')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:29:44.476881Z","iopub.status.busy":"2023-01-28T06:29:44.476149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","547/547 [==============================] - 54s 96ms/step - loss: 0.5552 - accuracy: 0.7615 - val_loss: 0.6948 - val_accuracy: 0.7328\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.73284, saving model to cervicalModel_weights.hdf5\n","Epoch 2/100\n","547/547 [==============================] - 53s 96ms/step - loss: 0.5235 - accuracy: 0.7783 - val_loss: 0.7151 - val_accuracy: 0.7218\n","\n","Epoch 00002: val_accuracy did not improve from 0.73284\n","Epoch 3/100\n","547/547 [==============================] - 53s 97ms/step - loss: 0.5196 - accuracy: 0.7810 - val_loss: 0.7059 - val_accuracy: 0.7328\n","\n","Epoch 00003: val_accuracy did not improve from 0.73284\n","Epoch 4/100\n","547/547 [==============================] - 52s 95ms/step - loss: 0.5141 - accuracy: 0.7776 - val_loss: 0.7000 - val_accuracy: 0.7267\n","\n","Epoch 00004: val_accuracy did not improve from 0.73284\n","Epoch 5/100\n","547/547 [==============================] - 53s 97ms/step - loss: 0.4906 - accuracy: 0.7923 - val_loss: 0.7340 - val_accuracy: 0.7132\n","\n","Epoch 00005: val_accuracy did not improve from 0.73284\n","Epoch 6/100\n","547/547 [==============================] - 53s 96ms/step - loss: 0.4789 - accuracy: 0.7927 - val_loss: 0.7329 - val_accuracy: 0.7145\n","\n","Epoch 00006: val_accuracy did not improve from 0.73284\n","Epoch 7/100\n","547/547 [==============================] - 51s 94ms/step - loss: 0.4736 - accuracy: 0.8031 - val_loss: 0.6930 - val_accuracy: 0.7255\n","\n","Epoch 00007: val_accuracy did not improve from 0.73284\n","Epoch 8/100\n","456/547 [========================>.....] - ETA: 8s - loss: 0.4595 - accuracy: 0.8003Epoch 10/100\n","547/547 [==============================] - 52s 96ms/step - loss: 0.4427 - accuracy: 0.8156 - val_loss: 0.8845 - val_accuracy: 0.6936\n","\n","Epoch 00010: val_accuracy did not improve from 0.73775\n","Epoch 11/100\n","547/547 [==============================] - 53s 96ms/step - loss: 0.4327 - accuracy: 0.8144 - val_loss: 0.7548 - val_accuracy: 0.7488\n","\n","Epoch 00011: val_accuracy improved from 0.73775 to 0.74877, saving model to cervicalModel_weights.hdf5\n","Epoch 12/100\n","547/547 [==============================] - 52s 95ms/step - loss: 0.4167 - accuracy: 0.8251 - val_loss: 0.7703 - val_accuracy: 0.7279\n","\n","Epoch 00012: val_accuracy did not improve from 0.74877\n","Epoch 13/100\n","547/547 [==============================] - 53s 96ms/step - loss: 0.4139 - accuracy: 0.8249 - val_loss: 0.7021 - val_accuracy: 0.7230\n","\n","Epoch 00013: val_accuracy did not improve from 0.74877\n"]}],"source":["# train model\n","history = model.fit(train_gen, steps_per_epoch= TRAIN_STEPS, validation_data=val_gen, validation_steps=VAL_STEPS, epochs= 100,\n","                   callbacks= [checkpoint, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_weights(\"cancer_screen_model_new.h5\")"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-01-28T06:29:30.939884Z","iopub.status.busy":"2023-01-28T06:29:30.938801Z","iopub.status.idle":"2023-01-28T06:29:33.675284Z","shell.execute_reply":"2023-01-28T06:29:33.674335Z","shell.execute_reply.started":"2023-01-28T06:29:30.939821Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["69/69 [==============================] - 2s 25ms/step - loss: 0.7036 - accuracy: 0.7360\n"]},{"data":{"text/plain":["[0.7035561800003052, 0.7360097169876099]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["model.load_weights(\"/kaggle/input/model-cervical/cervicalModel.weights.hdf5\")\n","model.evaluate(test_gen)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T18:06:08.117322Z","iopub.status.busy":"2023-01-27T18:06:08.116544Z","iopub.status.idle":"2023-01-27T18:06:10.439232Z","shell.execute_reply":"2023-01-27T18:06:10.438188Z","shell.execute_reply.started":"2023-01-27T18:06:08.117284Z"},"trusted":true},"outputs":[],"source":["# model.load_weights(\"/kaggle/input/prev-final-model/cancer_screen_model.h5\")\n","# model.save_weights(\"cancer_screen_model_new.h5\")\n","model.load_weights(\"cancer_screen_model.h5\")\n","# model.load_weights(\"cervicalModel_weights.hdf5\")\n","# model.evaluate(test_gen)\n","prediction = list(map(lambda x: np.argmax(x), model.predict(test_gen)))\n","labels = list(map(lambda x: int(x[-1]) - 1, test_labels))"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-01-27T18:40:54.844155Z","iopub.status.busy":"2023-01-27T18:40:54.843758Z","iopub.status.idle":"2023-01-27T18:40:54.851033Z","shell.execute_reply":"2023-01-27T18:40:54.849893Z","shell.execute_reply.started":"2023-01-27T18:40:54.844121Z"},"trusted":true},"outputs":[],"source":["def evaluate_image(path, model):\n","    img = cv2.imread(path)\n","    resized_img = cv2.resize(img, (180, 180))\n","    resized_img = tf.convert_to_tensor(resized_img)\n","    return np.argmax(model.predict(tf.reshape(resized_img, (1, resized_img.shape[0], resized_img.shape[1], resized_img.shape[2])))[0]) + 1"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
